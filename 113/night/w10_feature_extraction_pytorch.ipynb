{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPIlI5WStEOGxvxTiuT2mwB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **PyTorch Cats vs Dogs - Feature Extraction**\n","\n","---\n","\n","In this lesson, we learn how to use a pretrained network as a feature extractor. We'll then use those feautres as the input for our Logistic Regression Clasifier.\n","1. Load our pretrained VGG16 Model\n","2. Download our data and setup our transformations\n","3. Extract our Features using VGG16\n","4. Train a LR Classifier using those features\n","5. Run some inferences on our Test Data\n","---"],"metadata":{"id":"eaeb4P3T_zz7"}},{"cell_type":"markdown","source":["## **1. Download our Pre-trained Models (VGG16)**"],"metadata":{"id":"PzscuJ0S_6zr"}},{"cell_type":"code","source":["import torch\n","import os\n","import tqdm\n","import torch.nn as nn\n","import pandas as pd\n","from PIL import Image\n","from torchsummary import summary\n","from torchvision import datasets, transforms, models\n","import matplotlib.pyplot as plt\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = models.vgg16(pretrained=True)\n","model = model.to(device)\n","\n","summary(model, input_size = (3,224,224))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHhUKVlz_35H","executionInfo":{"status":"ok","timestamp":1699147988460,"user_tz":-480,"elapsed":24362,"user":{"displayName":"楊景明","userId":"02385702773581765739"}},"outputId":"1d176cc8-d8ec-4a64-8551-ad5445dc97bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 224, 224]           1,792\n","              ReLU-2         [-1, 64, 224, 224]               0\n","            Conv2d-3         [-1, 64, 224, 224]          36,928\n","              ReLU-4         [-1, 64, 224, 224]               0\n","         MaxPool2d-5         [-1, 64, 112, 112]               0\n","            Conv2d-6        [-1, 128, 112, 112]          73,856\n","              ReLU-7        [-1, 128, 112, 112]               0\n","            Conv2d-8        [-1, 128, 112, 112]         147,584\n","              ReLU-9        [-1, 128, 112, 112]               0\n","        MaxPool2d-10          [-1, 128, 56, 56]               0\n","           Conv2d-11          [-1, 256, 56, 56]         295,168\n","             ReLU-12          [-1, 256, 56, 56]               0\n","           Conv2d-13          [-1, 256, 56, 56]         590,080\n","             ReLU-14          [-1, 256, 56, 56]               0\n","           Conv2d-15          [-1, 256, 56, 56]         590,080\n","             ReLU-16          [-1, 256, 56, 56]               0\n","        MaxPool2d-17          [-1, 256, 28, 28]               0\n","           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n","             ReLU-19          [-1, 512, 28, 28]               0\n","           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n","             ReLU-21          [-1, 512, 28, 28]               0\n","           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n","             ReLU-23          [-1, 512, 28, 28]               0\n","        MaxPool2d-24          [-1, 512, 14, 14]               0\n","           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n","             ReLU-26          [-1, 512, 14, 14]               0\n","           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n","             ReLU-28          [-1, 512, 14, 14]               0\n","           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n","             ReLU-30          [-1, 512, 14, 14]               0\n","        MaxPool2d-31            [-1, 512, 7, 7]               0\n","AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n","           Linear-33                 [-1, 4096]     102,764,544\n","             ReLU-34                 [-1, 4096]               0\n","          Dropout-35                 [-1, 4096]               0\n","           Linear-36                 [-1, 4096]      16,781,312\n","             ReLU-37                 [-1, 4096]               0\n","          Dropout-38                 [-1, 4096]               0\n","           Linear-39                 [-1, 1000]       4,097,000\n","================================================================\n","Total params: 138,357,544\n","Trainable params: 138,357,544\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 218.78\n","Params size (MB): 527.79\n","Estimated Total Size (MB): 747.15\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["### **Remove the top Dense Fully Connected Layers**"],"metadata":{"id":"GYLpXf_5AEzS"}},{"cell_type":"code","source":["# remove last fully-connected layer\n","new_classifier = nn.Sequential(*list(model.classifier.children())[:-7]) # check out the star representaion in python\n","model.classifier = new_classifier"],"metadata":{"id":"jGS37jUgACK1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary(model, input_size = (3,224,224))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42hhW67RARS8","executionInfo":{"status":"ok","timestamp":1699148036901,"user_tz":-480,"elapsed":5,"user":{"displayName":"楊景明","userId":"02385702773581765739"}},"outputId":"22e2ed93-a27e-4c51-f7df-e018d7b6670b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 224, 224]           1,792\n","              ReLU-2         [-1, 64, 224, 224]               0\n","            Conv2d-3         [-1, 64, 224, 224]          36,928\n","              ReLU-4         [-1, 64, 224, 224]               0\n","         MaxPool2d-5         [-1, 64, 112, 112]               0\n","            Conv2d-6        [-1, 128, 112, 112]          73,856\n","              ReLU-7        [-1, 128, 112, 112]               0\n","            Conv2d-8        [-1, 128, 112, 112]         147,584\n","              ReLU-9        [-1, 128, 112, 112]               0\n","        MaxPool2d-10          [-1, 128, 56, 56]               0\n","           Conv2d-11          [-1, 256, 56, 56]         295,168\n","             ReLU-12          [-1, 256, 56, 56]               0\n","           Conv2d-13          [-1, 256, 56, 56]         590,080\n","             ReLU-14          [-1, 256, 56, 56]               0\n","           Conv2d-15          [-1, 256, 56, 56]         590,080\n","             ReLU-16          [-1, 256, 56, 56]               0\n","        MaxPool2d-17          [-1, 256, 28, 28]               0\n","           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n","             ReLU-19          [-1, 512, 28, 28]               0\n","           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n","             ReLU-21          [-1, 512, 28, 28]               0\n","           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n","             ReLU-23          [-1, 512, 28, 28]               0\n","        MaxPool2d-24          [-1, 512, 14, 14]               0\n","           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n","             ReLU-26          [-1, 512, 14, 14]               0\n","           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n","             ReLU-28          [-1, 512, 14, 14]               0\n","           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n","             ReLU-30          [-1, 512, 14, 14]               0\n","        MaxPool2d-31            [-1, 512, 7, 7]               0\n","AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n","================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 218.59\n","Params size (MB): 56.13\n","Estimated Total Size (MB): 275.29\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["## **2. Download our data and setup our Transformers**"],"metadata":{"id":"YPVrc1eaBrNc"}},{"cell_type":"code","source":["!wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/dogs-vs-cats.zip\n","!unzip -q dogs-vs-cats.zip\n","!unzip -q train.zip\n","!unzip -q test1.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buA0CIQ2BYXr","executionInfo":{"status":"ok","timestamp":1699146937253,"user_tz":-480,"elapsed":51061,"user":{"displayName":"楊景明","userId":"02385702773581765739"}},"outputId":"0c72edcc-b60e-4de8-cc60-c29efd73c4ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-05 01:14:45--  https://moderncomputervision.s3.eu-west-2.amazonaws.com/dogs-vs-cats.zip\n","Resolving moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)... 52.95.191.34, 3.5.245.136, 52.95.150.94, ...\n","Connecting to moderncomputervision.s3.eu-west-2.amazonaws.com (moderncomputervision.s3.eu-west-2.amazonaws.com)|52.95.191.34|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 851576689 (812M) [application/zip]\n","Saving to: ‘dogs-vs-cats.zip’\n","\n","dogs-vs-cats.zip    100%[===================>] 812.13M  34.5MB/s    in 25s     \n","\n","2023-11-05 01:15:11 (32.6 MB/s) - ‘dogs-vs-cats.zip’ saved [851576689/851576689]\n","\n"]}]},{"cell_type":"code","source":["# Set directory paths for our files\n","train_dir = './train'\n","test_dir = './test1'\n","\n","# Get files in our directories\n","train_files = os.listdir(train_dir)[:8000]\n","test_files = os.listdir(test_dir)\n","\n","print(f'Number of images in {train_dir} is {len(train_files)}')\n","print(f'Number of images in {test_dir} is {len(test_files)}')\n","\n","transformations = transforms.Compose([transforms.Resize((224,224)),\n","                                      transforms.ToTensor()])\n","\n","class Dataset():\n","    def __init__(self, filelist, filepath, transform = None):\n","        self.filelist = filelist\n","        self.filepath = filepath\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return int(len(self.filelist))\n","\n","    def __getitem__(self, index):\n","        imgpath = os.path.join(self.filepath, self.filelist[index])\n","        img = Image.open(imgpath)\n","\n","        if \"dog\" in imgpath:\n","            label = 1\n","        else:\n","            label = 0\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        return (img, label)\n","\n","# Create our train and test dataset objects\n","train = Dataset(train_files, train_dir, transformations)\n","val = Dataset(test_files, test_dir, transformations)\n","\n","# Create our dataloaders\n","train_dataset = torch.utils.data.DataLoader(dataset = train, batch_size = 32, shuffle=True)\n","val_dataset = torch.utils.data.DataLoader(dataset = val, batch_size = 32, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pm4OSPGVBtPe","executionInfo":{"status":"ok","timestamp":1699148212328,"user_tz":-480,"elapsed":305,"user":{"displayName":"楊景明","userId":"02385702773581765739"}},"outputId":"47b00268-b0f9-4056-ea0e-4c0965c37ca8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images in ./train is 8000\n","Number of images in ./test1 is 12500\n"]}]},{"cell_type":"markdown","source":["## **3. Extract our Features using VGG16**"],"metadata":{"id":"CvLxMiJlCZfy"}},{"cell_type":"code","source":["image_names = os.listdir(\"./train\")\n","image_paths = [\"./train/\"+ x for x in image_names]"],"metadata":{"id":"6h-BmZ8XCAA0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","model = model.to(device)\n","\n","with torch.no_grad():\n","    features = None\n","    image_labels = None\n","\n","    # loop over each batch and pass our input tensors to the model\n","    for data, label in tqdm.tqdm(train_dataset):\n","        x = data.to(device)\n","        output = model(x)\n","\n","        if features is not None:\n","            # Concatenates the given sequence of tensors in the given dimension.\n","            # cat needs at least two tensors so we only start to cat after the first loop\n","            features = torch.cat((features, output), 0)\n","            image_labels = torch.cat((image_labels, label), 0)\n","        else:\n","            features = output\n","            image_labels = label\n","\n","    # reshape our tensor to 25000 x 25088\n","    features = features.view(features.size(0), -1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QSM47wu0Cc4D","executionInfo":{"status":"ok","timestamp":1699148270724,"user_tz":-480,"elapsed":38666,"user":{"displayName":"楊景明","userId":"02385702773581765739"}},"outputId":"91698dc0-3b0f-4604-b59d-6d478bfea0cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 250/250 [00:38<00:00,  6.52it/s]\n"]}]},{"cell_type":"code","source":["# Check that we have features for all 25000 images\n","features.size(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UGM5S20TCyBS","executionInfo":{"status":"ok","timestamp":1699148280942,"user_tz":-480,"elapsed":332,"user":{"displayName":"楊景明","userId":"02385702773581765739"}},"outputId":"6f1cdf7d-3cbd-44d0-a396-7294772093e0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8000"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Check that we have labels for all 25000 images\n","image_labels.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Lw4E1KAFwgb","executionInfo":{"status":"ok","timestamp":1699148282090,"user_tz":-480,"elapsed":5,"user":{"displayName":"楊景明","userId":"02385702773581765739"}},"outputId":"c13f973c-60f6-433a-d0d7-b53dea7c5fb9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8000])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Check the shape to ensure our features are a flattened 512*7*7 array\n","features.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GOAuzQ3AGEcx","executionInfo":{"status":"ok","timestamp":1699148283163,"user_tz":-480,"elapsed":4,"user":{"displayName":"楊景明","userId":"02385702773581765739"}},"outputId":"266c53ba-3a9f-4d83-821a-46ac842b700d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8000, 25088])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## **4. Train a LR Classifier using those features**"],"metadata":{"id":"ga86EiPYGKwA"}},{"cell_type":"code","source":["# Convert our tensors to numpy arrays\n","features_np = features.cpu().numpy()\n","image_labels_np = image_labels.cpu().numpy()"],"metadata":{"id":"67uJvV3zGIsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","# Split our model into a test and training dataset to train our LR classifier\n","X_train, X_test, y_train, y_test = train_test_split(features_np, image_labels_np, test_size=0.2, random_state = 7)\n","\n","glm = LogisticRegression(C=0.1)\n","glm.fit(X_train,y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"id":"3rxDrcdJGR9j","executionInfo":{"status":"ok","timestamp":1699148323348,"user_tz":-480,"elapsed":30018,"user":{"displayName":"楊景明","userId":"02385702773581765739"}},"outputId":"2d1752a6-3c77-4e50-d452-b658e9bd64f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=0.1)"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.1)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Get Accruacy\n","accuracy = glm.score(X_test, y_test)\n","print(f'Accuracy on validation set using Logistic Regression: {accuracy*100}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16OKLmekGUx8","executionInfo":{"status":"ok","timestamp":1699148367710,"user_tz":-480,"elapsed":747,"user":{"displayName":"楊景明","userId":"02385702773581765739"}},"outputId":"8c9d9ee5-1424-4d2d-a68c-0d597ebb9aa4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on validation set using Logistic Regression: 96.625%\n"]}]},{"cell_type":"markdown","source":["## **5. Run some inferences on our Test Data**"],"metadata":{"id":"9ajSiHYeIsQO"}},{"cell_type":"code","source":["image_names_test = os.listdir(\"./test1\")\n","image_paths_test = [\"./test1/\"+ x for x in image_names_test]"],"metadata":{"id":"Ng7v7g9QG6fU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imsize = 224\n","\n","loader = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])\n","\n","def image_loader(loader, image_name):\n","    image = Image.open(image_name)\n","    image = loader(image)\n","    image = image.unsqueeze(0)\n","    return image"],"metadata":{"id":"eD32NLT0I3SP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","test_sample = random.sample(image_paths_test, 12)\n","model.eval()\n","\n","def test_img():\n","    result_lst = []\n","    for path in test_sample:\n","      image = image_loader(loader, path)\n","      output = model(image.to(device))\n","      output = output.cpu().detach().numpy()\n","      result = glm.predict(output)\n","      result = 'dog' if float(result) >0.5 else 'cat'\n","      result_lst.append(result)\n","    return result_lst"],"metadata":{"id":"ztsJGoujJHPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get test predictions from all models\n","pred_results = test_img()\n","pred_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Q_pHuxVJMCY","executionInfo":{"status":"ok","timestamp":1699148945060,"user_tz":-480,"elapsed":320,"user":{"displayName":"楊景明","userId":"02385702773581765739"}},"outputId":"d8bbc08a-9a0e-4b07-ac1f-b3b9ea779a05"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['cat',\n"," 'dog',\n"," 'dog',\n"," 'dog',\n"," 'dog',\n"," 'cat',\n"," 'cat',\n"," 'dog',\n"," 'dog',\n"," 'dog',\n"," 'cat',\n"," 'cat']"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["import cv2\n","\n","plt.figure(figsize=(15, 15))\n","\n","for i in range(0, 12):\n","    plt.subplot(4, 3, i+1)\n","    result = pred_results[i]\n","    img = test_sample[i]\n","    image = cv2.imread(img)\n","    image = cv2.resize(image, (224, 224), interpolation = cv2.INTER_CUBIC)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    plt.text(72, 248, f'Feature Extractor CNN: {result}', color='lightgreen',fontsize= 12, bbox=dict(facecolor='black', alpha=0.9))\n","    plt.imshow(image)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"14KBuMDSHDx_WKB0-qQw6MFibJTM0lFXt"},"id":"CiJk_6TgJPK0","executionInfo":{"status":"ok","timestamp":1699148980798,"user_tz":-480,"elapsed":28422,"user":{"displayName":"楊景明","userId":"02385702773581765739"}},"outputId":"651ab764-772f-400a-9cc2-0f6ca1c1d75a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"uOWXdUorJSX-"},"execution_count":null,"outputs":[]}]}