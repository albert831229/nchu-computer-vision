{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPBgten4nK7cT2Mq9Ms5Bg3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Introduction to PyTorch**\n","### **Training a Simple CNN on the MNIST Dataset - Handwrittent Digits**\n","\n","1. Import PyTorch library and functions\n","2. Define our Transformer\n","3. Load our dataset\n","4. Inspect and Visualization our image dataset\n","5. Create our Data Loader for load batches of images\n","6. Building our Model\n","7. Training our Model\n","8. Analyizing it's Accuracy\n","9. Saving our Model\n","10. Plotting our training logs"],"metadata":{"id":"aJRF8s2voEKR"}},{"cell_type":"markdown","source":["### **1. Import our libaries and modules**\n","\n","We import PyTorch by importing ```torch```. We'll be using **torchvision** which is a PyTorch package that consists of popular datasets, model acrhitectures and common image transformations."],"metadata":{"id":"AU7Z8zllobPI"}},{"cell_type":"code","source":["# Import PyTorch\n","import torch\n","\n","# We use torchvision to get our dataset and useful image transformations\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","# Import PyTorch's optimization libary and nn\n","# nn is used as the basic building block for our Network graphs\n","import torch.optim as optim\n","import torch.nn as nn\n","\n","# Are we using our GPU?\n","print(\"GPU available: {}\".format(torch.cuda.is_available()))"],"metadata":{"id":"DCdNS8kzoJQS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### If GPU is available set device = ```'cuda'``` if not set device = ```'cpu'```"],"metadata":{"id":"igeTKTlUpUxM"}},{"cell_type":"code","source":["if torch.cuda.is_available():\n","  device = 'cuda'\n","else:\n","  device = 'cpu'"],"metadata":{"id":"Fz1M-AhdoiCh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2. We define our transformer**\n","\n","Transfomers are needed to cast the image data into the required format for input into our model.\n","\n","- It's composed using the ```transforms.Compose``` function\n","- We chain the commands or instructions for our pipeline as the arguements\n","- We use ```transforms.ToTensor()``` to convert the image data into a PyTorch Tensor\n","- We use ```transforms.Normalize()``` to normalize our pixel values\n","- By passing the input as ```(0.5, ), (0.5,)``` we Normalize our image data between -1 and +1\n","- Note for RGB images we use ```transformed.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))``` instead\n","\n","**NOTE**:\n","Our raw pixel values in our MNIST dataset range from 0 to 255. Each image is 28 pixels heigh and 28 pixels wide, with a depth of 1 as it's grayscale.\n","\n","**Why Normalize?**\n","\n","1. To ensure all features, or in our case, pixel intensities, are weighted equally when training our CNN\n","2. Makes training faster as it avoids oscilations during training\n","3. Removes and bias or skewness in our image data\n","\n","\n","**Why 0.5?**\n","\n","Normalization is done like this:\n","\n","`image = (image - mean) / std`\n","\n","Using the parameters 0.5,0.5 sets the Mean and STD to 0.5. Using the formula above this gives us:\n","\n","- Min value = `(0-0.5)/0.5 = 1`\n","- Max value = `(1-0.5)/0.5 = -1`\n","\n","For color images we use a tuple of (0.5,0.5,0.5) to set the Mean of the RGB channels to 0.5 and another tuple of (0.5, 0.5, 0.5) to set the STD to 0.5"],"metadata":{"id":"WlAAkvFKp3gh"}},{"cell_type":"code","source":["# Transform to a PyTorch tensors and the normalize our valeus between -1 and +1\n","transform = transforms.Compose([transforms.ToTensor(),\n","                               transforms.Normalize((0.5, ), (0.5, )) ])"],"metadata":{"id":"F3pcetz1pXNb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **3. Fetch our MNIST Dataset using torchvision**\n","\n","- View other datasets that can be accesed via torchvision here - https://pytorch.org/vision/stable/datasets.html"],"metadata":{"id":"FiIJX4ims2RD"}},{"cell_type":"code","source":["# Load our Training Data and specify what transform to use when loading\n","trainset = torchvision.datasets.MNIST('mnist',\n","                                      train = True,\n","                                      download = True,\n","                                      transform = transform)\n","\n","# Load our Test Data and specify what transform to use when loading\n","testset = torchvision.datasets.MNIST('mnist',\n","                                     train = False,\n","                                     download = True,\n","                                     transform = transform)"],"metadata":{"id":"RtKrdkurs0Mb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **About Training and Test Data**\n","\n","There are two subsets of the data being used here:\n","\n","- **Training data** Data that is used to optimize model parameters (used during training)\n","- **Test/Validation data** Data that is used to evaluate the model performance\n","\n","During training, we monitor model performance on the test data.\n","\n","**Good Machine Learning Practice**\n","\n","Often we keep another **test set** for testing the final model in order to get an unbiased estimate of *out of sample* accuracy.\n","\n","However, MNIST doesn't have a separate test set. Therefore, we use the test set for both validation and test.\n"],"metadata":{"id":"xieL-8IGtgXo"}},{"cell_type":"markdown","source":["### **4. Let's inpsect a sample of our training data**\n","\n","\n","Let's inspect our training and test dataset dimensions."],"metadata":{"id":"lIdAZYSatmFI"}},{"cell_type":"code","source":["# We have 60,000 Image samples for our training data & 10,000 for our test data\n","# each 28 x 28 pixels, as they are grayscale, there is no 3rd dimension to our image\n","print(trainset.data.shape)\n","print(testset.data.shape)"],"metadata":{"id":"FSa_LW4OtmiI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Let's look at the an Individual Sample of Data**\n","\n","You will see that our data has not yet been normalized between -1 and 1."],"metadata":{"id":"RMSonG6Ltx0k"}},{"cell_type":"code","source":["# This is the first value in our dataset\n","print(trainset.data[0].shape)\n","print(trainset.data[0])"],"metadata":{"id":"Fm8arXM-tuB-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **We can use matplotlib to show many examples from our dataset**"],"metadata":{"id":"zvC7o6riuBOC"}},{"cell_type":"code","source":["# Let's view the 50 first images of the MNIST training dataset\n","import matplotlib.pyplot as plt\n","\n","figure = plt.figure()\n","num_of_images = 50\n","\n","for index in range(1, num_of_images + 1):\n","    plt.subplot(5, 10, index)\n","    plt.axis('off')\n","    plt.imshow(trainset.data[index], cmap='gray_r')"],"metadata":{"id":"lfd7NvPHt6bW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **5. Create our Data Loader**\n","\n","A **Data Loader** is a function that we'll use to grab our data in specified batch sizes (we'll use 128) during training.\n","\n","Remember we can't feed all our data through the network at once, therefore that is why we split data into batches.\n","\n","We set **shuffle** equal to True to prevent data sequence bias. For example, in some datasets the each class in usually in order, so to avoid loading batches of only a single class, we shuffle our data."],"metadata":{"id":"1qTY1t4HuTmJ"}},{"cell_type":"code","source":["# Prepare train and test loader\n","trainloader = torch.utils.data.DataLoader(trainset,\n","                                           batch_size = 128,\n","                                           shuffle = True)\n","\n","testloader = torch.utils.data.DataLoader(testset,\n","                                          batch_size = 128,\n","                                          shuffle = False)"],"metadata":{"id":"ICsgOi9LuHOC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Using Iter and Next() for load batches**\n"],"metadata":{"id":"Z0oN2XyCwEGM"}},{"cell_type":"code","source":["# We use the Python function iter to return an iterator for our train_loader object\n","dataiter = iter(trainloader)\n","\n","# We use next to get the first batch of data from our iterator\n","images, labels = next(dataiter)\n","\n","print(images.shape)\n","print(labels.shape)"],"metadata":{"id":"8e7sl2ZXuxMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images[0].shape"],"metadata":{"id":"gwtjLaSQwRl5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **6. Now we build our Model**\n","\n","We will use the ```nn.Sequential``` method to construct our model. Alernatively we can use the functional module, however this is simpler and more similar to styles you'll work with in Keras.\n","\n","### **Building a Convolution Filter Layer**\n","\n","```\n","nn.Conv2d(in_channels=1,\n","          out_channels=32,\n","          kernel_size=3,\n","          stride=1,\n","          padding=1)\n","```\n","\n","- **in_channels (int)** — This is the number of channels in the input image (for grayscale images use 1 and for RGB color images use 3)\n","- **out_channels (int)** — This is the number of channels produced by the convolution. We use 32 channels or 32 filters. **NOTE** 32 will be the number of **in_channels** in the next network layer.\n","- **kernel_size (int or tuple)** — This is the size of the convolving kernel. We use 3 here, which gives a kernel size of 3 x 3.\n","- **stride (int or tuple, optional)** — Stride of the convolution. (Default: 1)\n","- **padding (int or tuple, optional)** — Zero-padding added to both sides of the input (Default: 0). We use a padding = 1.\n","\n","### **The Max Pool Layer**\n","\n","- Each pooling layer i.e., nn.MaxPool2d(2, 2) halves both the height and the width of the image, so by using 2 pooling layers, the height and width are 1/4 of the original sizes.\n","\n","![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/CleanShot%202020-11-29%20at%204.21.04%402x.png)"],"metadata":{"id":"648WSuV0w_MV"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F #\n","\n","# Create our Model using a Python Class\n","class Net(nn.Module):\n","    def __init__(self):\n","        # super is a subclass of the nn.Module and inherits all its methods\n","        super(Net, self).__init__()\n","\n","        # We define our layer objects here\n","        # Our first CNN Layer using 32 Fitlers of 3x3 size, with stride of 1 & padding of 0\n","        self.conv1 = nn.Conv2d(1, 32, 3)\n","        # Our second CNN Layer using 64 Fitlers of 3x3 size, with stride of 1 & padding of 0\n","        self.conv2 = nn.Conv2d(32, 64, 3)\n","        # Our Max Pool Layer 2 x 2 kernel of stride 2\n","        self.pool = nn.MaxPool2d(2, 2)\n","        # Our first Fully Connected Layer (called Linear), takes the output of our Max Pool\n","        # which is 12 x 12 x 64 and connects it to a set of 128 nodes\n","        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n","        # Our second Fully Connected Layer, connects the 128 nodes to 10 output nodes (our classes)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        # here we define our forward propogation sequence\n","        # Remember it's Conv1 - Relu - Conv2 - Relu - Max Pool - Flatten - FC1 - FC2\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 64 * 12 * 12) # Flatten\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Create an instance of the model and move it (memory and operations) to the CUDA device\n","net = Net()\n","net.to(device)"],"metadata":{"id":"4P7oj8hIw5II"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Printing out our Model**"],"metadata":{"id":"qJHGIG-bzN2w"}},{"cell_type":"code","source":["print(net)"],"metadata":{"id":"AEdF0Np3zJwn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **7. Defining a Loss Function and Optimizer**\n","\n","We need to define what type of loss we'll be using and what method will be using to update the gradients.\n","1. We use Cross Entropy Loss as it is a multi-class problem\n","2. We use Stochastic Gradient Descent (SGD) - we also specify a learn rate (LR) of 0.001 and momentum 0.9"],"metadata":{"id":"yjGaUTOFzbLZ"}},{"cell_type":"code","source":["# We import our optimizer function\n","import torch.optim as optim\n","\n","# We use Cross Entropy Loss as our loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# For our gradient descent algorthim or Optimizer\n","# We use Stochastic Gradient Descent (SGD) with a learning rate of 0.001\n","# We set the momentum to be 0.9\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"],"metadata":{"id":"aUETSFINzP9t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **8. Training Our Model**\n","\n","In PyTorch we use the building block functions to execute the training algorithm that we should be somewhat familar with by now.\n","\n","![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/CleanShot%202020-11-29%20at%207.04.32%402x.png)"],"metadata":{"id":"ntd9B4BY19rT"}},{"cell_type":"code","source":["# Will take a while to run!!!!!!!!!!\n","# We loop over the traing dataset multiple times (each time is called an epoch)\n","epochs = 10\n","\n","# Create some empty arrays to store logs\n","epoch_log = []\n","loss_log = []\n","accuracy_log = []\n","\n","# Iterate for a specified number of epochs\n","for epoch in range(epochs):\n","    print(f'Starting Epoch: {epoch+1}...')\n","\n","    # We keep adding or accumulating our loss after each mini-batch in running_loss\n","    running_loss = 0.0\n","\n","    # We iterate through our trainloader iterator\n","    # Each cycle is a minibatch\n","    for i, data in enumerate(trainloader):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","\n","        # Move our data to GPU\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        # Clear the gradients before training by setting to zero\n","        # Required for a fresh start\n","        optimizer.zero_grad()\n","\n","        # Forward -> backprop + optimize\n","        outputs = net(inputs) # Forward Propagation\n","        loss = criterion(outputs, labels) # Get Loss (quantify the difference between the results and predictions)\n","        loss.backward() # Back propagate to obtain the new gradients for all nodes\n","        optimizer.step() # Update the gradients/weights\n","\n","        # Print Training statistics - Epoch/Iterations/Loss/Accuracy\n","        running_loss += loss.item()\n","\n","        if i % 50 == 49:    # show our loss every 50 mini-batches\n","            correct = 0 # Initialize our variable to hold the count for the correct predictions\n","            total = 0 # Initialize our variable to hold the count of the number of labels iterated\n","\n","            # We don't need gradients for validation, so wrap in\n","            # no_grad to save memory\n","            with torch.no_grad():\n","                # Iterate through the testloader iterator\n","                for data in testloader:\n","                    images, labels = data\n","                    # Move our data to GPU\n","                    images = images.to(device)\n","                    labels = labels.to(device)\n","\n","                    # Foward propagate our test data batch through our model\n","                    outputs = net(images)\n","\n","                     # Get predictions from the maximum value of the predicted output tensor\n","                     # we set dim = 1 as it specifies the number of dimensions to reduce\n","                    _, predicted = torch.max(outputs.data, dim = 1)\n","                    # Keep adding the label size or length to the total variable\n","                    total += labels.size(0)\n","                    # Keep a running total of the number of predictions predicted correctly\n","                    correct += (predicted == labels).sum().item()\n","\n","                accuracy = 100 * correct / total\n","                epoch_num = epoch + 1\n","                actual_loss = running_loss / 50\n","                print(f'Epoch: {epoch_num}, Mini-Batches Completed: {(i+1)}, Loss: {actual_loss:.3f}, Test Accuracy = {accuracy:.3f}%')\n","                running_loss = 0.0\n","\n","    # Store training stats after each epoch\n","    epoch_log.append(epoch_num)\n","    loss_log.append(actual_loss)\n","    accuracy_log.append(accuracy)\n","\n","print('Finished Training')"],"metadata":{"id":"V_9uO_LOziXL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **9. Saving Our Model**\n","\n","We use the ```torch.save()``` function to save our model.\n","\n","```net.state_dict()``` saves our model weights in a dictionay format."],"metadata":{"id":"AQxAZhMy-toM"}},{"cell_type":"code","source":["PATH = './mnist_cnn_net.pth'\n","torch.save(net.state_dict(), PATH)"],"metadata":{"id":"2QdkS1OY9ahN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Let's look at some images from your Test Data and view their Ground Truth labels**"],"metadata":{"id":"cjq2pSV3-_xO"}},{"cell_type":"code","source":["# Loading one mini-batch\n","dataiter = iter(testloader)\n","images, labels = next(dataiter)\n","\n","for index in range(len(images)):\n","    plt.subplot(5, 26, index + 1)\n","    plt.axis('off')\n","    plt.imshow(images[index][0], cmap='gray_r')\n","print('GroundTruth: ',''.join('%1s' % labels[j].numpy() for j in range(128)))"],"metadata":{"id":"Tzx3dDYu-yql"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Let's reload the model we just saved**"],"metadata":{"id":"7rgkfJJTBVBF"}},{"cell_type":"code","source":["# Create an instance of the model and move it (memory and operations) to the CUDA device.\n","net = Net()\n","net.to(device)\n","\n","# Load weights from the specified path\n","net.load_state_dict(torch.load(PATH))"],"metadata":{"id":"fCKqJaqBAIUV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Getting Predictions**\n","\n","Note when working with tensors on the GPU, we have to convert it back to a numpy array to perform python operations on it.\n","\n","```your_tensor.cpu().numpy()```"],"metadata":{"id":"Iy5l3YjUBadA"}},{"cell_type":"code","source":["## Let's forward propagate one mini-batch and get the predicted outputs\n","# We use the Python function iter to return an iterator for our train_loader object\n","test_iter = iter(testloader)\n","\n","# We use next to get the first batch of data from our iterator\n","images, labels = next(test_iter)\n","\n","# Move our data to GPU\n","images = images.to(device)\n","labels = labels.to(device)\n","\n","outputs = net(images)\n","\n","# Get the class predictions using torch.max\n","_, predicted = torch.max(outputs, 1)\n","\n","# Print our 128 predictions\n","print('Predicted: ', ''.join('%1s' % predicted[j].cpu().numpy() for j in range(128)))"],"metadata":{"id":"UpUKVFT0BXxo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Showing our Test Accuracy again**"],"metadata":{"id":"Gvz6QyBmBpr2"}},{"cell_type":"code","source":["correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        # Move our data to GPU\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Accuracy of the network on the 10000 test images: {accuracy:.3}%')"],"metadata":{"id":"Lcwj3vxYBkuG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **10. Plotting our Training Logs**\n","\n","Remember we created some lists to log our training stats?\n","\n","```\n","# Create some empty arrays to store logs\n","epoch_log = []\n","loss_log = []\n","accuracy_log = []\n","```\n","\n","**Let's now plot those logs**"],"metadata":{"id":"Z1Y9m0NeBzDJ"}},{"cell_type":"code","source":["# To create a plot with secondary y-axis we need to create a subplot\n","fig, ax1 = plt.subplots()\n","\n","# Set title and x-axis label rotation\n","plt.title(\"Accuracy & Loss vs Epoch\")\n","plt.xticks(rotation=45)\n","\n","# We use twinx to create a plot a secondary y axis\n","ax2 = ax1.twinx()\n","\n","# Create plot for loss_log and accuracy_log\n","ax1.plot(epoch_log, loss_log, 'g-')\n","ax2.plot(epoch_log, accuracy_log, 'b-')\n","\n","# Set labels\n","ax1.set_xlabel('Epochs')\n","ax1.set_ylabel('Loss', color='g')\n","ax2.set_ylabel('Test Accuracy', color='b')\n","\n","plt.show()"],"metadata":{"id":"zbjh7LmkBuYP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**NOTE**\n","\n","```net.eval()``` is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn off them during model evaluation, and .eval() will do it for you. In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation:"],"metadata":{"id":"qmrqREhDDE1w"}},{"cell_type":"markdown","source":["# Error analysis"],"metadata":{"id":"aqk5qQNSG7ks"}},{"cell_type":"code","source":["import numpy as np\n","\n","def imgshow(title=\"\", image = None, size = 6):\n","    w, h = image.shape[0], image.shape[1]\n","    aspect_ratio = w/h\n","    plt.figure(figsize=(size * aspect_ratio,size))\n","    plt.imshow(image, cmap='gray_r')\n","    plt.title(title)\n","    plt.show()\n","\n","net.eval()\n","\n","# We don't need gradients for validation, so wrap in\n","# no_grad to save memory\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","\n","        # Move our data to GPU\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Get our outputs\n","        outputs = net(images)\n","\n","        # use torch.argmax() to get the predictions, argmax is used for long_tensors\n","        predictions = torch.argmax(outputs, dim=1)\n","\n","        # For test data in each batch we identify when predictions did not match the label\n","        # then we print out the actual ground truth\n","        for i in range(data[0].shape[0]):\n","            pred = predictions[i].item()\n","            label = labels[i]\n","            if(label != pred):\n","                print(f'Actual Label: {label}, Predicted Label: {pred}')\n","                img = np.reshape(images[i].cpu().numpy(),[28,28])\n","                imgshow(\"\", img, size = 1)"],"metadata":{"id":"kOtT64XkB1Tk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","nb_classes = 10\n","\n","# Initialize the prediction and label lists(tensors)\n","predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n","lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n","\n","with torch.no_grad():\n","    for i, (inputs, classes) in enumerate(testloader):\n","        inputs = inputs.to(device)\n","        classes = classes.to(device)\n","        outputs = net(inputs)\n","        _, preds = torch.max(outputs, 1)\n","\n","        # Append batch prediction results\n","        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n","        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n","\n","# Confusion matrix\n","conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n","print(conf_mat)\n","\n","# Per-class accuracy\n","class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n","print(class_accuracy)"],"metadata":{"id":"Xc-HDuzwDG09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Rh8HMZq8E8Kb"},"execution_count":null,"outputs":[]}]}